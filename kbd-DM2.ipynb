{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import urllib\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nexturl(url):\n",
    "    \n",
    "    \"\"\" identifies and returns the first link on the wikipedia page currently being explored \"\"\"\n",
    "    \"\"\" relies on the idea that the 'first link' of a wikipedia page is always in a paragraph of text \"\"\"\n",
    "    \"\"\" links found earlier in the html content are thus not legit and must be discarded \"\"\"\n",
    "    \"\"\" the logic thus consists in isolating paragraphs, then search for the first link in there \"\"\"\n",
    "    \n",
    "    # first return webpage content\n",
    "    data = requests.get(url)\n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    \n",
    "    # obtain page title\n",
    "    title = soup.title.string.replace(' — Wikipédia',\" \").strip().lower()\n",
    "    \n",
    "    # initialise paragraph validity at false\n",
    "    validity = False\n",
    "    \n",
    "    # then look for paragraphs\n",
    "    paragraphs = soup.select('p')\n",
    "    \n",
    "    # and loop over them\n",
    "    for paragraph in paragraphs:\n",
    "        # test for validity: validity is reached when actual text article starts\n",
    "        # which always begins with a recall of the title\n",
    "        if title in paragraph.text.lower():\n",
    "            validity = True\n",
    "        # if paragraphs are valid, search may start\n",
    "        if validity == True:\n",
    "            if paragraph.find(\"a\", recursive=True):\n",
    "                link = paragraph.find(\"a\", recursive=True).get('href')\n",
    "                return urllib.parse.urljoin('https://fr.wikipedia.org/', link)\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://fr.wikipedia.org/wiki/End%C3%A9misme\n"
     ]
    }
   ],
   "source": [
    "print(nexturl(initial_url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def continue_jumping(url, url_stop, jumps):\n",
    "    \n",
    "    \"\"\"check whether jumps may go on or not\"\"\"\n",
    "    \n",
    "    if url == url_stop:\n",
    "        print(\"French Wikipedia page for philosophy eventually found! Now stop jumping like mad.\")\n",
    "        return False\n",
    "    elif not url:\n",
    "        print(\"No link found on previous jump. Philosophy will have to wait. I'm going for a beer.\")\n",
    "        return False\n",
    "    elif jumps>=100:\n",
    "        print(\"Already a hundred jumps and philosophy still not reached. I'm no kangaroo. I quit.\")\n",
    "        return False\n",
    "    else:\n",
    "        print(\"Jump number \" + str(jumps) + \" here. Going on jumping.\")\n",
    "        return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jump number 1 here. Going on jumping.\n",
      "Jump number 2 here. Going on jumping.\n",
      "Jump number 3 here. Going on jumping.\n",
      "Jump number 4 here. Going on jumping.\n",
      "Jump number 5 here. Going on jumping.\n",
      "No link found on previous jump. Philosophy will have to wait. I'm going for a beer.\n",
      "https://fr.wikipedia.org/wiki/Boulangerie\n",
      "https://fr.wikipedia.org/wiki/Commerce_de_d%C3%A9tail\n",
      "https://fr.wikipedia.org/wiki/Commerce\n",
      "https://fr.wikipedia.org/wiki/Activit%C3%A9_%C3%A9conomique\n",
      "https://fr.wikipedia.org/wiki/Comptabilit%C3%A9s_nationales\n",
      "https://fr.wikipedia.org/wiki/%C3%89conomie_(activit%C3%A9_humaine)\n"
     ]
    }
   ],
   "source": [
    "# source and target urls\n",
    "url_source = 'https://fr.wikipedia.org/wiki/Boulangerie'\n",
    "url_stop = 'https://fr.wikipedia.org/wiki/Philosophie'\n",
    "\n",
    "# initiate list of urls, number of jumps, and jump continuation\n",
    "url=url_source\n",
    "url_list = dict()\n",
    "url_list['0'] = url\n",
    "jumps = 0\n",
    "go_on_jumping = True\n",
    "\n",
    "while go_on_jumping:\n",
    "    url = nexturl(url)\n",
    "    jumps+=1\n",
    "    url_list[str(jumps)] = url\n",
    "    go_on_jumping = continue_jumping(url, url_stop, jumps)\n",
    "\n",
    "for i in range(jumps):\n",
    "    print(url_list[str(i)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
